{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VINOTH1916/offi/blob/main/Copy_of_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8893df7",
      "metadata": {
        "id": "d8893df7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Memory Usage Function\n",
        "def memory_usage():\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 ** 2\n",
        "\n",
        "# Load Dataset\n",
        "data = pd.read_csv('/content/sample_data/balanced_ddos_data.csv')  # Replace with the actual path to your dataset\n",
        "\n",
        "# Fix Column Names: Remove leading/trailing spaces\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Encode 'Label': BENIGN -> 0, Other Attack Types -> 1\n",
        "data['Label'] = data['Label'].map(lambda x: 0 if x == 'BENIGN' else 1)\n",
        "\n",
        "# Data Preprocessing\n",
        "X = data.drop(['Label', 'Source IP', 'Destination IP'], axis=1).astype(np.float32)  # Drop irrelevant columns\n",
        "y = data['Label']\n",
        "\n",
        "# Handle Missing Values: Impute with column mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Convert features to sparse matrix for efficiency\n",
        "X = csr_matrix(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "start_time = time.time()\n",
        "clf = ExtraTreesClassifier(n_estimators=100, max_depth=20, max_features='sqrt', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Training Metrics\n",
        "y_pred_train = clf.predict(X_train)\n",
        "train_cm = confusion_matrix(y_train, y_pred_train)\n",
        "train_cr = classification_report(y_train, y_pred_train)\n",
        "train_acc = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "# Testing Metrics\n",
        "y_pred_test = clf.predict(X_test)\n",
        "test_cm = confusion_matrix(y_test, y_pred_test)\n",
        "test_cr = classification_report(y_test, y_pred_test)\n",
        "test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Cross-validation for Bias and Variance Estimation\n",
        "predicted = cross_val_predict(clf, X, y, cv=5)\n",
        "bias = np.mean(predicted - y)\n",
        "variance = np.var(predicted)\n",
        "\n",
        "# Results\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Training Confusion Matrix:\\n\", train_cm)\n",
        "print(\"Training Classification Report:\\n\", train_cr)\n",
        "print(\"Testing Accuracy:\", test_acc)\n",
        "print(\"Testing Confusion Matrix:\\n\", test_cm)\n",
        "print(\"Testing Classification Report:\\n\", test_cr)\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "print(\"Memory Usage:\", memory_usage(), \"MB\")\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Variance:\", variance)\n",
        "\n",
        "# Optional: Calculate AUC-ROC\n",
        "if len(set(y)) == 2:  # Only calculate if it's binary classification\n",
        "    y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    print(\"AUC-ROC Score:\", auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# capture_network.py\n",
        "import psutil\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated capture (replace with actual traffic monitoring logic)\n",
        "def capture_network_data():\n",
        "    network_data = {\n",
        "        'Source IP': ['192.168.1.1'],\n",
        "        'Destination IP': ['8.8.8.8'],\n",
        "        'Protocol': [17],  # Example: 17 for UDP, replace with actual protocols\n",
        "        'Flow Duration': [1000],  # Example\n",
        "        'Total Length of Fwd Packets': [1500],\n",
        "        'Total Length of Bwd Packets': [0],\n",
        "        'Flow IAT Mean': [10],\n",
        "        'Flow IAT Max': [50],\n",
        "        'Flow IAT Min': [1],\n",
        "        'Packet Length Mean': [60],\n",
        "        'FIN Flag Count': [0],\n",
        "        'SYN Flag Count': [0],\n",
        "        'PSH Flag Count': [0],\n",
        "        'ACK Flag Count': [1]\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(network_data)\n",
        "\n",
        "# Main function to capture data\n",
        "if __name__ == \"__main__\":\n",
        "    captured_data = capture_network_data()\n",
        "    captured_data.to_csv('captured_network_data.csv', index=False)\n",
        "    print(\"Captured data saved to 'captured_network_data.csv'.\")\n"
      ],
      "metadata": {
        "id": "rjPFzu3UenbE"
      },
      "id": "rjPFzu3UenbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_data.py\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Function to encode IP addresses (simple version)\n",
        "def encode_ip(df):\n",
        "    # Convert IP addresses into a simple numerical representation using their hashed values or first octet\n",
        "    df['Source IP'] = df['Source IP'].apply(lambda x: sum([int(i) for i in x.split('.')]))  # Sum of octets as an example\n",
        "    df['Destination IP'] = df['Destination IP'].apply(lambda x: sum([int(i) for i in x.split('.')]))  # Same here\n",
        "    return df\n",
        "\n",
        "# Preprocess data (scale and handle missing values)\n",
        "def preprocess_data(df):\n",
        "    df = encode_ip(df)  # Encode IP columns\n",
        "    df = df.fillna(0)  # Handle missing data\n",
        "\n",
        "    # Ensure that all data is numeric\n",
        "    df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Standardize the data (scaling)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)  # Apply the same scaling as used during training\n",
        "    return pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "# Load captured data\n",
        "captured_data = pd.read_csv('captured_network_data.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "processed_data = preprocess_data(captured_data)\n",
        "processed_data.to_csv('processed_data.csv', index=False)\n",
        "print(\"Processed data saved to 'processed_data.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6ErovZreou-",
        "outputId": "8d15759e-6a33-4f95-a83d-cdeb4b0bbf9d"
      },
      "id": "w6ErovZreou-",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to 'processed_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the trained model\n",
        "clf = joblib.load('trained_model.joblib')\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess_data(df):\n",
        "    # Encode IP columns (example)\n",
        "    # Converting IP addresses to integers (you can use ipaddress module for better handling)\n",
        "    df['Source IP'] = df['Source IP'].apply(lambda x: sum([int(i) for i in x.split('.')]))\n",
        "    df['Destination IP'] = df['Destination IP'].apply(lambda x: sum([int(i) for i in x.split('.')]))\n",
        "\n",
        "    # Handle missing values and ensure all data is numeric\n",
        "    df = df.fillna(0)\n",
        "    df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Scaling the data (using the same scaler as in training)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "    return pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "# Load the captured data for prediction\n",
        "captured_data = pd.read_csv('captured_network_data.csv')\n",
        "\n",
        "# Preprocess the captured data\n",
        "processed_data = preprocess_data(captured_data)\n",
        "\n",
        "# Strip any extra spaces in column names and ensure the relevant columns\n",
        "processed_data.columns = processed_data.columns.str.strip()\n",
        "\n",
        "# Select the same columns as used during training (12 features)\n",
        "processed_data = processed_data[['Flow Duration',\n",
        "                                 'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
        "                                 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min',\n",
        "                                 'Packet Length Mean', 'FIN Flag Count',\n",
        "                                 'SYN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'Protocol']]\n",
        "\n",
        "# Convert DataFrame to numpy array (model input format)\n",
        "processed_data_np = processed_data.to_numpy()\n",
        "\n",
        "# Run prediction on the processed data\n",
        "predictions = clf.predict(processed_data_np)\n",
        "\n",
        "# Print the predictions (you can map 0 and 1 to more descriptive labels)\n",
        "predicted_labels = ['BENIGN' if pred == 0 else 'DDoS' for pred in predictions]\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "id": "sJKat-LYezux"
      },
      "id": "sJKat-LYezux",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# monitor_network.py\n",
        "import time\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from capture_network import capture_network_data  # Your network capture function\n",
        "from preprocess_data import preprocess_data  # Your preprocessing function\n",
        "\n",
        "# detect_ddos.py\n",
        "# detect_ddos.py\n",
        "def detect_ddos(clf, data):\n",
        "    # Predict using the trained classifier\n",
        "    predictions = clf.predict(data)\n",
        "\n",
        "    # Map predictions to human-readable labels\n",
        "    predicted_labels = ['BENIGN' if pred == 0 else 'DDoS' for pred in predictions]\n",
        "\n",
        "    # Print predictions for each record\n",
        "    for i, label in enumerate(predicted_labels):\n",
        "        print(f\"Prediction for record {i+1}: {label}\")\n",
        "    print(f\"Total predictions: {len(predicted_labels)}\")\n",
        "\n",
        "# monitor_network.py\n",
        "def monitor_network():\n",
        "    while True:\n",
        "        # Capture network data (replace with real-time capture logic)\n",
        "        captured_data = capture_network_data()  # This function should return the captured data\n",
        "\n",
        "        if captured_data is None or captured_data.empty:\n",
        "            print(\"No network data captured. Skipping prediction cycle.\")\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "\n",
        "        # Preprocess the captured data\n",
        "        processed_data = preprocess_data(captured_data)\n",
        "\n",
        "        # Ensure the processed data has exactly 12 features (as in the training)\n",
        "        processed_data.columns = processed_data.columns.str.strip()  # Remove any spaces from column names\n",
        "        processed_data = processed_data[['Flow Duration',\n",
        "                                         'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
        "                                         'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min',\n",
        "                                         'Packet Length Mean', 'FIN Flag Count',\n",
        "                                         'SYN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'Protocol']]\n",
        "\n",
        "        # Convert DataFrame to numpy array (model input format)\n",
        "        processed_data_np = processed_data.to_numpy()\n",
        "\n",
        "        # Detect DDoS attack using pre-trained model\n",
        "        detect_ddos(clf, processed_data_np)\n",
        "\n",
        "        # Sleep for 5 seconds before checking again\n",
        "        time.sleep(5)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    monitor_network()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "dg1muJ3Ye8_S",
        "outputId": "58fca5c1-63b9-4d29-a24e-cb285c619415"
      },
      "id": "dg1muJ3Ye8_S",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n",
            "Prediction for record 1: BENIGN\n",
            "Total predictions: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-a4cea0ca6347>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmonitor_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-a4cea0ca6347>\u001b[0m in \u001b[0;36mmonitor_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Sleep for 5 seconds before checking again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "03c2b54f",
      "metadata": {
        "id": "03c2b54f",
        "outputId": "86f3e853-9a9c-43a4-a099-8282c02ba4d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9974401419867911\n",
            "Training Confusion Matrix:\n",
            " [[90886   360]\n",
            " [   90 84455]]\n",
            "Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     91246\n",
            "           1       1.00      1.00      1.00     84545\n",
            "\n",
            "    accuracy                           1.00    175791\n",
            "   macro avg       1.00      1.00      1.00    175791\n",
            "weighted avg       1.00      1.00      1.00    175791\n",
            "\n",
            "Testing Accuracy: 0.9972467461545463\n",
            "Testing Confusion Matrix:\n",
            " [[22499    83]\n",
            " [   38 21328]]\n",
            "Testing Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     22582\n",
            "           1       1.00      1.00      1.00     21366\n",
            "\n",
            "    accuracy                           1.00     43948\n",
            "   macro avg       1.00      1.00      1.00     43948\n",
            "weighted avg       1.00      1.00      1.00     43948\n",
            "\n",
            "Training Time: 31.288391590118408 seconds\n",
            "Memory Usage: 453.9453125 MB\n",
            "Bias: 0.0013698069072854615\n",
            "Variance: 0.24972295240655223\n",
            "AUC-ROC Score: 0.9999062016202003\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Memory Usage Function\n",
        "def memory_usage():\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 ** 2\n",
        "\n",
        "# Load Dataset\n",
        "data = pd.read_csv('/content/sample_data/balanced_ddos_data.csv')  # Replace with the actual path to your dataset\n",
        "\n",
        "# Fix Column Names: Remove leading/trailing spaces\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Encode 'Label': BENIGN -> 0, Other Attack Types -> 1\n",
        "data['Label'] = data['Label'].map(lambda x: 0 if x == 'BENIGN' else 1)\n",
        "\n",
        "# Data Preprocessing\n",
        "X = data.drop(['Label', 'Source IP', 'Destination IP'], axis=1).astype(np.float32)  # Drop irrelevant columns\n",
        "y = data['Label']\n",
        "\n",
        "# Handle Missing Values: Impute with column mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Convert features to sparse matrix for efficiency\n",
        "X = csr_matrix(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "start_time = time.time()\n",
        "clf = ExtraTreesClassifier(n_estimators=100, max_depth=20, max_features='sqrt', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Training Metrics\n",
        "y_pred_train = clf.predict(X_train)\n",
        "train_cm = confusion_matrix(y_train, y_pred_train)\n",
        "train_cr = classification_report(y_train, y_pred_train)\n",
        "train_acc = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "# Testing Metrics\n",
        "y_pred_test = clf.predict(X_test)\n",
        "test_cm = confusion_matrix(y_test, y_pred_test)\n",
        "test_cr = classification_report(y_test, y_pred_test)\n",
        "test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Cross-validation for Bias and Variance Estimation\n",
        "predicted = cross_val_predict(clf, X, y, cv=5)\n",
        "bias = np.mean(predicted - y)\n",
        "variance = np.var(predicted)\n",
        "\n",
        "# Results\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Training Confusion Matrix:\\n\", train_cm)\n",
        "print(\"Training Classification Report:\\n\", train_cr)\n",
        "print(\"Testing Accuracy:\", test_acc)\n",
        "print(\"Testing Confusion Matrix:\\n\", test_cm)\n",
        "print(\"Testing Classification Report:\\n\", test_cr)\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "print(\"Memory Usage:\", memory_usage(), \"MB\")\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Variance:\", variance)\n",
        "\n",
        "# Optional: Calculate AUC-ROC\n",
        "if len(set(y)) == 2:  # Only calculate if it's binary classification\n",
        "    y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    print(\"AUC-ROC Score:\", auc)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}